{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "js_build",
   "metadata": {
    "id": "js_build"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run([\"npm\", \"i\", \"--no-progress\"], cwd=\"../widgets\", check=True)\n",
    "subprocess.run([\"npm\", \"i\", \"--no-progress\"], cwd=\"widget-wrappers\", check=True)\n",
    "subprocess.run([\"bash\", \"build_wrapped_widgets.sh\"], cwd=\"widget-wrappers\", check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from constants import hue_range, latent_dim, sidelength, size_range\n",
    "from dataset import generate_dataset\n",
    "from elbo import approximate_elbo\n",
    "from model import VAE\n",
    "from util import BatchIterator, onnx_export, onnx_export_to_files, plot_losses\n",
    "from vaewidgets import (\n",
    "    AreaSelectionWidget,\n",
    "    dataset_explanation,\n",
    "    dataset_visualization,\n",
    "    decoding,\n",
    "    mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_explanation",
   "metadata": {
    "id": "dataset_explanation"
   },
   "source": [
    "## Dataset explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_explanation_code",
   "metadata": {
    "id": "dataset_explanation_code"
   },
   "outputs": [],
   "source": [
    "dataset_explanation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_validatation_set_split",
   "metadata": {
    "id": "train_validatation_set_split"
   },
   "source": [
    "## Train/validation set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valset_selection",
   "metadata": {
    "id": "valset_selection"
   },
   "outputs": [],
   "source": [
    "valset_selection = AreaSelectionWidget(size_range, hue_range, \"Size\", \"Hue\", 0.6, 0.4, 0.3, 0.3)\n",
    "valset_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_generation",
   "metadata": {
    "id": "dataset_generation"
   },
   "outputs": [],
   "source": [
    "trainset_coords, valset_coords, trainset, valset = generate_dataset(\n",
    "    size_range=size_range,\n",
    "    hue_range=hue_range,\n",
    "    valset_size_range=(valset_selection.x, valset_selection.x + valset_selection.width),\n",
    "    valset_hue_range=(valset_selection.y, valset_selection.y + valset_selection.height),\n",
    "    num_samples=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_visualization(trainset_coords, valset_coords, trainset, valset, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c3244",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0bba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "vae = VAE(latent_dim=2).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "train_losses: list[float] = []\n",
    "val_losses: list[float] = []\n",
    "best_val_loss: float = np.inf\n",
    "\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "pbar = trange(num_epochs)\n",
    "for epoch in pbar:\n",
    "    vae.train()\n",
    "    per_batch_train_losses = []\n",
    "    batch_iterator = BatchIterator(trainset, batch_size)\n",
    "    for batch in batch_iterator:\n",
    "        x = (batch / 255.0).to(device)\n",
    "        mu_x, logvar_x, _, mu_z = vae(x)\n",
    "        loss: torch.Tensor = -approximate_elbo(\n",
    "            x.view(x.shape[0], sidelength * sidelength * 3),\n",
    "            mu_z.view(mu_z.shape[0], sidelength * sidelength * 3),\n",
    "            mu_x,\n",
    "            logvar_x,\n",
    "            sigma2=1.0,\n",
    "        ).mean()\n",
    "        per_batch_train_losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  # type: ignore[no-untyped-call]\n",
    "        optimizer.step()\n",
    "    train_losses.append(float(np.mean(per_batch_train_losses)))\n",
    "\n",
    "    per_batch_val_losses = []\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_iterator = BatchIterator(valset, batch_size)\n",
    "        for batch in batch_iterator:\n",
    "            x = (batch / 255.0).to(device)\n",
    "            mu_x, logvar_x, _, mu_z = vae(x)\n",
    "            loss = -approximate_elbo(\n",
    "                x.view(x.shape[0], sidelength * sidelength * 3),\n",
    "                mu_z.view(mu_z.shape[0], sidelength * sidelength * 3),\n",
    "                mu_x,\n",
    "                logvar_x,\n",
    "                sigma2=1.0,\n",
    "            ).mean()\n",
    "            per_batch_val_losses.append(loss.item())\n",
    "    pbar.set_description(\n",
    "        f\"Train Loss: {train_losses[-1]:.4f}, Val Loss: {np.mean(per_batch_val_losses):.4f}\"\n",
    "    )\n",
    "    epoch_val_loss = float(np.mean(per_batch_val_losses))\n",
    "    val_losses.append(float(epoch_val_loss))\n",
    "\n",
    "    if epoch > float(num_epochs) * 0.75 and epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        torch.save(vae.state_dict(), \"vae.pth\")\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model again and export to ONNX so we can use it in the browser\n",
    "vae = VAE(latent_dim=latent_dim)\n",
    "vae.load_state_dict(torch.load(\"vae.pth\"))\n",
    "vae.eval()\n",
    "\n",
    "encoder, decoder = onnx_export(vae.encoder, vae.decoder)\n",
    "\n",
    "# TODO Remove\n",
    "onnx_export_to_files(vae.encoder, vae.decoder, \"encoder.onnx\", \"decoder.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdaa6d-5b70-4784-aa1e-209b7e8659a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    (\n",
    "        (valset_selection.x, valset_selection.x + valset_selection.width),\n",
    "        (valset_selection.y, valset_selection.y + valset_selection.height),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding(encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
