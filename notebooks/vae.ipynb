{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904adcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo if needed (e.g. on Colab)\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def is_correct_repo() -> bool:\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"remote\", \"get-url\", \"origin\"], capture_output=True, text=True, check=True\n",
    "        )\n",
    "        remote_url = result.stdout.strip()\n",
    "        return remote_url in [\n",
    "            \"https://github.com/mariogemoll/vae.git\",\n",
    "            \"git@github.com:mariogemoll/vae.git\",\n",
    "        ]\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return False\n",
    "\n",
    "\n",
    "if not is_correct_repo():\n",
    "    !git clone https://github.com/mariogemoll/vae.git\n",
    "\n",
    "if not os.getcwd().endswith(\"vae/notebooks\"):\n",
    "    %cd vae/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd90a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies\n",
    "%pip install -r requirements-build.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "js_build",
   "metadata": {
    "id": "js_build"
   },
   "outputs": [],
   "source": [
    "# Build UI widgets\n",
    "subprocess.run([\"npm\", \"i\", \"--no-progress\"], cwd=\"../widgets\", check=True)\n",
    "subprocess.run([\"npm\", \"i\", \"--no-progress\"], cwd=\"widget-wrappers\", check=True)\n",
    "subprocess.run([\"bash\", \"build_wrapped_widgets.sh\"], cwd=\"widget-wrappers\", check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from constants import hue_range, latent_dim, sidelength, size_range\n",
    "from dataset import generate_dataset\n",
    "from grid import make_standard_grid\n",
    "from image import get_images\n",
    "from model import VAE\n",
    "from training import train\n",
    "from util import get_device, onnx_export, plot_losses\n",
    "from vaewidgets import (\n",
    "    AreaSelectionWidget,\n",
    "    dataset_explanation,\n",
    "    dataset_visualization,\n",
    "    decoding,\n",
    "    evolution,\n",
    "    mapping,\n",
    "    sampling,\n",
    ")\n",
    "\n",
    "device = get_device()\n",
    "if device.type == \"cpu\":\n",
    "    print(\"Using CPU for training. You might want to switch to a GPU to speed things up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_explanation",
   "metadata": {
    "id": "dataset_explanation"
   },
   "source": [
    "## Dataset explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_explanation_code",
   "metadata": {
    "id": "dataset_explanation_code"
   },
   "outputs": [],
   "source": [
    "dataset_explanation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_validatation_set_split",
   "metadata": {
    "id": "train_validatation_set_split"
   },
   "source": [
    "## Train/validation set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valset_selection",
   "metadata": {
    "id": "valset_selection"
   },
   "outputs": [],
   "source": [
    "valset_selection = AreaSelectionWidget(size_range, hue_range, \"Size\", \"Hue\", 0.6, 0.4, 0.3, 0.3)\n",
    "valset_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_generation",
   "metadata": {
    "id": "dataset_generation"
   },
   "outputs": [],
   "source": [
    "trainset_coords, valset_coords, trainset, valset = generate_dataset(\n",
    "    size_range=size_range,\n",
    "    hue_range=hue_range,\n",
    "    valset_size_range=(valset_selection.x, valset_selection.x + valset_selection.width),\n",
    "    valset_hue_range=(valset_selection.y, valset_selection.y + valset_selection.height),\n",
    "    num_samples=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_visualization(trainset_coords, valset_coords, trainset, valset, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c3244",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0bba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "standard_grid = make_standard_grid(size_range, hue_range)\n",
    "imgs = get_images(sidelength, [tuple(pair) for pair in standard_grid.reshape(-1, 2).tolist()])\n",
    "grid_x = (torch.from_numpy(imgs).float() / 255.0).to(device)\n",
    "train_losses, val_losses, grids = train(device, trainset, valset, \"vae.pth\", 100, 256, 64, grid_x)\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model again and export to ONNX so we can use it in the browser\n",
    "vae = VAE(latent_dim=latent_dim)\n",
    "vae.load_state_dict(torch.load(\"vae.pth\"))\n",
    "vae.eval()\n",
    "encoder, decoder = onnx_export(vae.encoder, vae.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31053e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334886f-ce2f-499d-8491-ea112ec346fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert grids is not None\n",
    "evolution(train_losses, val_losses, grids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdaa6d-5b70-4784-aa1e-209b7e8659a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    (\n",
    "        (valset_selection.x, valset_selection.x + valset_selection.width),\n",
    "        (valset_selection.y, valset_selection.y + valset_selection.height),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding(encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae-neu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
